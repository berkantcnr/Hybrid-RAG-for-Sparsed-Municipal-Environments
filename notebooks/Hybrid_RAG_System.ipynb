{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16012840",
   "metadata": {},
   "source": [
    "# **HYBRID RAG SYSTEM for Sparse Municipal Environments:** \n",
    "This is the base code for our thesis, we're doing everything below, data preparing, embeddings, knowledge graph connections, running the hybrid system and testing the Hybrid RAG system performance via answering the competency questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6855b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a178b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 1: SETUP \n",
    "# ==============================================================================\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. ROOT_DIR \n",
    "current_dir = os.getcwd()\n",
    "ROOT_DIR = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "# 3. Dynamic Folder Paths\n",
    "DATA_PATH = os.path.join(ROOT_DIR, \"data\", \"municipal_pdfs\")\n",
    "ASSETS_DIR = os.path.join(ROOT_DIR, \"assets\")\n",
    "FONT_PATH = os.path.join(ASSETS_DIR, \"Roboto-Regular.ttf\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\" ROOT_DIR: {ROOT_DIR}\")\n",
    "print(f\" PDF Path: {DATA_PATH}\")\n",
    "print(f\" Font Path: {FONT_PATH}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982070ad",
   "metadata": {},
   "source": [
    "# **DATA PREPARATION PART: **\n",
    "In municipal  domains, data is mostly unstructured and sparsed across various web portals. To address this lack of structured datasets and working effective indexing within our Vector Database, a custom data  pipeline was established.\n",
    "\n",
    "The source has 14 primary institutional websites (Parent Links). A recursive web scraping algorithm was developed to traverse these domains and their associated sub-pages, resulting in a total of 53 processed URLs.\n",
    "\n",
    "Data extraction was executed using the **BeautifulSoup4** library. A rigorous data cleaning phase followed, aimed at removing noise and unnecessary web elements—such as cookie consent banners, social media widgets, and navigation to ensure high-quality textual input.\n",
    "\n",
    "The cleaned data was subsequently converted into standardized PDF documents using the implementation provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 2: DATA INGESTION & PDF GENERATION\n",
    "# DESCRIPTION: Scrapes target municipal websites, cleans the textual content,\n",
    "# and converts structured data into standardized PDF documents for RAG ingestion.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from fpdf import FPDF\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. CONFIGURATION & PATHS\n",
    "# ------------------------------------------------------------------------------\n",
    "# Paths are derived dynamically from the ROOT_DIR set in the previous cell.\n",
    "# If ROOT_DIR is not defined, we calculate it relative to this notebook.\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "PDF_STORAGE_PATH = os.path.join(ROOT_DIR, \"data\", \"municipal_pdfs\")\n",
    "ASSETS_DIR = os.path.join(ROOT_DIR, \"assets\")\n",
    "FONT_PATH = os.path.join(ASSETS_DIR, \"Roboto-Regular.ttf\")\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(PDF_STORAGE_PATH, exist_ok=True)\n",
    "os.makedirs(ASSETS_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. TARGET URLs DEFINITION\n",
    "# ------------------------------------------------------------------------------\n",
    "TARGETS = [\n",
    "    # --- DIVERSITY MEDIA ---\n",
    "    {\"name\": \"DIVERSITY_MEDIA\", \"url\": \"https://diversitymedia.info/media-workspace\"},\n",
    "    {\"name\": \"DIVERSITY_MEDIA\", \"url\": \"https://diversitymedia.info/orga\"},\n",
    "    {\"name\": \"DIVERSITY_MEDIA\", \"url\": \"https://diversitymedia.info/kontakt\"},\n",
    "\n",
    "    # --- KUNSTKULTURQUARTIER ---\n",
    "    {\"name\": \"KUNSTKULTUR\", \"url\": \"https://www.kunstkulturquartier.de/werkstaetten\"},\n",
    "    {\"name\": \"KUNSTKULTUR\", \"url\": \"https://www.kunstkulturquartier.de/kuenstlerhaus/haus/kontakt-1\"},\n",
    "\n",
    "    # --- HEIZHAUS ---\n",
    "    {\"name\": \"HEIZHAUS\", \"url\": \"https://www.heizhaus.org/das-haus\"},\n",
    "    {\"name\": \"HEIZHAUS\", \"url\": \"https://www.heizhaus.org/kontakt\"},\n",
    "    {\"name\": \"HEIZHAUS\", \"url\": \"https://www.heizhaus.org/\"},\n",
    "\n",
    "    # --- LEIHLA (Bluepingu) ---\n",
    "    {\"name\": \"LEIHLA\", \"url\": \"https://leihla.bluepingu.de/\"},\n",
    "    {\"name\": \"LEIHLA\", \"url\": \"https://leihla.bluepingu.de/nutzungsbedingungen/\"},\n",
    "    {\"name\": \"LEIHLA\", \"url\": \"https://leihla.bluepingu.de/cb_itemgallery/?itemcat=werkzeug-allgemein\"},\n",
    "    {\"name\": \"LEIHLA\", \"url\": \"https://leihla.bluepingu.de/cb_itemgallery/?itemcat=technik\"},\n",
    "    {\"name\": \"LEIHLA\", \"url\": \"https://leihla.bluepingu.de/cb_itemgallery/?itemcat=werkzeug-textil\"},\n",
    "\n",
    "    # --- ESSBARE STADT ---\n",
    "    {\"name\": \"ESSBARE_STADT\", \"url\": \"https://essbare-stadt-nuernberg.de/fair-share/\"},\n",
    "    {\"name\": \"ESSBARE_STADT\", \"url\": \"https://leihbar.bluepingu.de/leihkatalog_essbare_stadt/\"},\n",
    "    {\"name\": \"ESSBARE_STADT\", \"url\": \"https://essbare-stadt-nuernberg.de/#kontakt\"},\n",
    "\n",
    "    # --- FABLAB NÜRNBERG ---\n",
    "    {\"name\": \"FABLAB_NBG\", \"url\": \"https://fablab-nuernberg.de/\"},\n",
    "    {\"name\": \"FABLAB_NBG\", \"url\": \"https://fablab-nuernberg.de/ueber-uns/der-verein\"},\n",
    "    {\"name\": \"FABLAB_NBG\", \"url\": \"https://fablab-nuernberg.de/ueber-uns/raeumlichkeiten\"},\n",
    "    {\"name\": \"FABLAB_NBG\", \"url\": \"https://fablab-nuernberg.de/ueber-uns/geraete\"},\n",
    "\n",
    "    # --- HOLZWERKSTATT GOSTENHOF ---\n",
    "    {\"name\": \"HOLZWERKSTATT\", \"url\": \"http://holzwerkstatt-gostenhof.de/\"},\n",
    "    {\"name\": \"HOLZWERKSTATT\", \"url\": \"http://holzwerkstatt-gostenhof.de/maschinen/\"},\n",
    "    {\"name\": \"HOLZWERKSTATT\", \"url\": \"http://holzwerkstatt-gostenhof.de/mitgliedschaft/\"},\n",
    "    {\"name\": \"HOLZWERKSTATT\", \"url\": \"http://holzwerkstatt-gostenhof.de/faq/\"},\n",
    "\n",
    "    # --- LEONARDO ---\n",
    "    {\"name\": \"LEONARDO\", \"url\": \"https://leonardo-zentrum.de/labs/\"},\n",
    "    {\"name\": \"LEONARDO\", \"url\": \"https://leonardo-zentrum.de/labs/makerspace-werkstatt/\"},\n",
    "    {\"name\": \"LEONARDO\", \"url\": \"https://leonardo-zentrum.de/labs/ar-vr-labor-studio/\"},\n",
    "    {\"name\": \"LEONARDO\", \"url\": \"https://leonardo-zentrum.de/labs/miracl-soundlabor-tonstudio/\"},\n",
    "    {\"name\": \"LEONARDO\", \"url\": \"https://leonardo-zentrum.de/labs/eventspace-co-working-space/\"},\n",
    "    {\"name\": \"LEONARDO\", \"url\": \"https://leonardo-zentrum.de/kontakt/\"},\n",
    "    {\"name\": \"LEONARDO\", \"url\": \"https://leonardo-zentrum.de/ueber-uns/\"},\n",
    "\n",
    "    # --- FABLAB NÜLAND ---\n",
    "    {\"name\": \"FABLAB_NUELAND\", \"url\": \"https://fablab.nueland.de/\"},\n",
    "    {\"name\": \"FABLAB_NUELAND\", \"url\": \"https://fablab.nueland.de/index.php/wir-ueber-uns\"},\n",
    "    {\"name\": \"FABLAB_NUELAND\", \"url\": \"https://fablab.nueland.de/index.php/das-fablab\"},\n",
    "    {\"name\": \"FABLAB_NUELAND\", \"url\": \"https://fablab.nueland.de/index.php/mach-mit\"},\n",
    "    {\"name\": \"FABLAB_NUELAND\", \"url\": \"https://fablab.nueland.de/index.php/kontakt\"},\n",
    "\n",
    "    # --- KOLEO ---\n",
    "    {\"name\": \"KOLEO\", \"url\": \"https://www.iska-nuernberg.de/koleo/\"},\n",
    "    {\"name\": \"KOLEO\", \"url\": \"https://www.iska-nuernberg.de/koleo/kontakt.html\"},\n",
    "\n",
    "    # --- KLARA ---\n",
    "    {\"name\": \"KLARA\", \"url\": \"https://www.nuernberg.de/internet/nuernberg_engagiert/klara.html\"},\n",
    "\n",
    "    # --- OHM LAB ---\n",
    "    {\"name\": \"OHM_LAB\", \"url\": \"https://www.th-nuernberg.de/einrichtungen-gesamt/administration-und-service/lehr-und-kompetenzentwicklung/lehr-und-lernraeume/ohmlab-maker-und-coworking-space/\"},\n",
    "\n",
    "    # --- FABLAB FAU ---\n",
    "    {\"name\": \"FABLAB_FAU\", \"url\": \"https://fablab.fau.de/\"},\n",
    "    {\"name\": \"FABLAB_FAU\", \"url\": \"https://fablab.fau.de/tool/lasercutter/\"},\n",
    "    {\"name\": \"FABLAB_FAU\", \"url\": \"https://fablab.fau.de/tool/3d-drucker/\"},\n",
    "    {\"name\": \"FABLAB_FAU\", \"url\": \"https://fablab.fau.de/tool/schneideplotter/\"},\n",
    "    {\"name\": \"FABLAB_FAU\", \"url\": \"https://fablab.fau.de/tool/zerspanung/cnc-fraese/\"},\n",
    "    {\"name\": \"FABLAB_FAU\", \"url\": \"https://fablab.fau.de/tool/zerspanung/cnc-drehbank/\"},\n",
    "    {\"name\": \"FABLAB_FAU\", \"url\": \"https://fablab.fau.de/elektrowerkzeuge/\"},\n",
    "    {\"name\": \"FABLAB_FAU\", \"url\": \"https://fablab.fau.de/tool/handwerkzeuge/\"},\n",
    "    {\"name\": \"FABLAB_FAU\", \"url\": \"https://fablab.fau.de/tool/textilbearbeitung/naehmaschine/\"},\n",
    "    {\"name\": \"FABLAB_FAU\", \"url\": \"https://fablab.fau.de/tool/textilbearbeitung/stickmaschine/\"},\n",
    "    {\"name\": \"FABLAB_FAU\", \"url\": \"https://fablab.fau.de/tool/textilbearbeitung/textilpresse/\"},\n",
    "    {\"name\": \"FABLAB_FAU\", \"url\": \"https://fablab.fau.de/tool/multifunktionstisch/\"},\n",
    "    {\"name\": \"FABLAB_FAU\", \"url\": \"https://fablab.fau.de/kontakt/\"},\n",
    "\n",
    "    # --- ODL ---\n",
    "    {\"name\": \"ODL_TOLLWERK\", \"url\": \"https://odl-nbg.de/de/\"}\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. HELPER FUNCTIONS: CLEANING & UTILS\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_junk_list():\n",
    "    \"\"\"Returns a list of keywords and phrases to exclude during scraping.\"\"\"\n",
    "    return [\n",
    "        # Navigation\n",
    "        \"home\", \"startseite\", \"menu\", \"menü\", \"hauptmenü\", \"untermenü\",\n",
    "        \"navigation\", \"breadcrumb\", \"you are here\", \"sie sind hier\",\n",
    "        \"suche\", \"search\", \"suchen\", \"lupe\", \"leiste öffnen\",\n",
    "        \"zum inhalt springen\", \"skip to content\", \"zur hauptnavigation\",\n",
    "        \"navigation ausklappen\", \"top of page\", \"bottom of page\",\n",
    "        # Footer & Legal\n",
    "        \"impressum\", \"datenschutz\", \"privacy\", \"disclaimer\", \"haftungsausschluss\",\n",
    "        \"agb\", \"nutzungsbedingungen\", \"copyright\", \"alle rechte vorbehalten\",\n",
    "        \"powered by\", \"theme by\", \"wordpress\", \"secured by miniorange\",\n",
    "        # Cookie Consent\n",
    "        \"gdpr\", \"cookie\", \"cookies\", \"unbedingt notwendige cookies\",\n",
    "        \"einstellungen speichern\", \"alle aktivieren\", \"deaktiviert\", \"aktiviert\",\n",
    "        \"cookie-informationen\", \"cookie-einstellungen\",\n",
    "        # Actions & Auth\n",
    "        \"login\", \"anmelden\", \"register\", \"registrieren\", \"logout\", \"abmelden\",\n",
    "        \"warenkorb\", \"cart\", \"kasse\", \"checkout\", \"mein konto\",\n",
    "        \"passwort vergessen\", \"remember me\", \"mehr erfahren\", \"weiterlesen\",\n",
    "        # Social Media\n",
    "        \"instagram\", \"facebook\", \"youtube\", \"twitter\", \"linkedin\", \"rss\", \"feed\",\n",
    "        \"envelope\", \"google+\", \"xing\",\n",
    "        # Accessibility\n",
    "        \"barrierefreiheit\", \"text vergrößern\", \"graustufen\", \"kontrast\",\n",
    "        \"hoher kontrast\", \"heller modus\", \"links unterstreichen\", \"lesbare schriftart\",\n",
    "        \"nach oben\", \"top\", \"reset\", \"text verkleinern\", \"schriftgröße\"\n",
    "    ]\n",
    "\n",
    "def clean_text(text, url):\n",
    "    \"\"\"\n",
    "    Cleans raw HTML text by removing boilerplate, navigation elements, \n",
    "    and institution-specific sidebars.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    cleaned_lines = []\n",
    "    junk_exact = get_junk_list()\n",
    "\n",
    "    # Specific sidebar content for FAU FabLab\n",
    "    fau_sidebar = [\n",
    "        \"was ist ein fablab\", \"wie werde ich fablab-betreuer:in?\", \"termine\",\n",
    "        \"ausstattung\", \"maschinen im überblick\", \"preise\", \"bilder\", \"projekte\",\n",
    "        \"projekte unserer besucher\", \"forschungs- und abschlussarbeiten\",\n",
    "        \"project group diybio - build your own biotech lab\", \"english\"\n",
    "    ]\n",
    "\n",
    "    for line in lines:\n",
    "        original = line.strip()\n",
    "        lower_line = original.lower()\n",
    "\n",
    "        if not original: continue\n",
    "        if lower_line in junk_exact: continue\n",
    "\n",
    "        # Fix specific glitch in Bluepingu/Leihla site\n",
    "        if \"leihla\" in url:\n",
    "            if re.search(r'\\d{10,}', original):\n",
    "                original = re.sub(r'\\d{10,}', ' ', original).strip()\n",
    "                lower_line = original.lower()\n",
    "                if not original or original in [\"Fürth\", \"Marktplatz\"]:\n",
    "                    continue\n",
    "\n",
    "        # Filter out file markers and pagination\n",
    "        if re.match(r'^\\d+$', original): continue\n",
    "        if re.search(r'\\(PDF, \\d+ KB\\)', original): continue\n",
    "        if original.startswith(\"<\") and original.endswith(\">\"): continue\n",
    "\n",
    "        # Partial matching filters\n",
    "        if \"cookie\" in lower_line and (\"verwend\" in lower_line or \"einstellung\" in lower_line): continue\n",
    "        if \"gdpr\" in lower_line: continue\n",
    "        if \"instagram.com\" in lower_line or \"facebook.com\" in lower_line: continue\n",
    "        if \"source:\" in lower_line: continue\n",
    "        if \"internetverbindung abgebrochen\" in lower_line: continue\n",
    "        if \"spambots geschützt\" in lower_line: continue\n",
    "\n",
    "        # Filter FabLab Nürnberg Navigation\n",
    "        if \"fablab-nuernberg\" in url:\n",
    "            if any(x in lower_line for x in [\"openlab\", \"kidslab\", \"repaircafé\", \"textilelab\"]) and len(original) < 25:\n",
    "                continue\n",
    "\n",
    "        # Filter FAU FabLab Sidebar\n",
    "        if \"fablab.fau\" in url:\n",
    "            if lower_line in fau_sidebar: continue\n",
    "            if len(original) < 40 and any(x in lower_line for x in [\"3d-drucker\", \"lasercutter\", \"schneideplotter\", \"elektronik\", \"textilbearbeitung\", \"zerspanung\"]):\n",
    "                # Context check: preserve if the page itself is about that topic\n",
    "                is_current_topic = False\n",
    "                if \"lasercutter\" in lower_line and \"lasercutter\" in url: is_current_topic = True\n",
    "                if \"3d-drucker\" in lower_line and \"3d-drucker\" in url: is_current_topic = True\n",
    "                if \"schneideplotter\" in lower_line and \"schneideplotter\" in url: is_current_topic = True\n",
    "                if \"elektronik\" in lower_line and \"elektro\" in url: is_current_topic = True\n",
    "                if \"textil\" in lower_line and \"textil\" in url: is_current_topic = True\n",
    "                if \"zerspanung\" in lower_line and \"zerspanung\" in url: is_current_topic = True\n",
    "\n",
    "                if not is_current_topic:\n",
    "                    continue\n",
    "\n",
    "        cleaned_lines.append(original)\n",
    "\n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"Fetches the URL and returns a BeautifulSoup object.\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Network issue with {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_pdf(text, url, filename):\n",
    "    \"\"\"Generates a PDF from the cleaned text using the Roboto font.\"\"\"\n",
    "    # Ensure font exists, download if necessary\n",
    "    if not os.path.exists(FONT_PATH):\n",
    "        print(f\"[INFO] Roboto font not found at {FONT_PATH}. Downloading...\")\n",
    "        font_url = \"https://github.com/google/fonts/raw/main/ofl/roboto/Roboto-Regular.ttf\"\n",
    "        r = requests.get(font_url, allow_redirects=True)\n",
    "        with open(FONT_PATH, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print(\"[SUCCESS] Font downloaded.\")\n",
    "\n",
    "    try:\n",
    "        pdf = FPDF()\n",
    "        pdf.add_page()\n",
    "        pdf.add_font('Roboto', '', FONT_PATH)\n",
    "        pdf.set_font('Roboto', '', 8)\n",
    "        pdf.set_text_color(100, 100, 100)\n",
    "        pdf.cell(0, 10, f\"Source: {url}\", new_x=\"LMARGIN\", new_y=\"NEXT\") # Updated for FPDF2\n",
    "        pdf.ln(5)\n",
    "        pdf.set_font('Roboto', '', 11)\n",
    "        pdf.set_text_color(0, 0, 0)\n",
    "        \n",
    "        # Safe encode/decode to handle non-latin characters roughly\n",
    "        safe_text = text.encode('utf-8', 'replace').decode('utf-8')\n",
    "        pdf.multi_cell(0, 6, safe_text)\n",
    "        pdf.output(filename)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to generate PDF for {url}: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. EXECUTION PIPELINE\n",
    "# ------------------------------------------------------------------------------\n",
    "def execute_data_ingestion():\n",
    "    print(f\"Saving PDFs to: {PDF_STORAGE_PATH}\")\n",
    "    \n",
    "    # Clean up existing PDF directory to ensure fresh data\n",
    "    if os.path.exists(PDF_STORAGE_PATH):\n",
    "        shutil.rmtree(PDF_STORAGE_PATH)\n",
    "    os.makedirs(PDF_STORAGE_PATH)\n",
    "\n",
    "    processed_urls = set()\n",
    "    count = 0\n",
    "\n",
    "    # Expand targets dynamically (e.g., Essbare Stadt catalogs)\n",
    "    expanded_targets = list(TARGETS)\n",
    "    for item in TARGETS:\n",
    "        if \"leihkatalog_essbare_stadt\" in item['url']:\n",
    "            print(\"Expanding Essbare Stadt Catalog...\")\n",
    "            soup = get_soup(item['url'])\n",
    "            if soup:\n",
    "                for a in soup.find_all('a', href=True):\n",
    "                    href = a['href']\n",
    "                    if \"itemcat\" in href:\n",
    "                        full_url = urljoin(item['url'], href)\n",
    "                        expanded_targets.append({\"name\": \"ESSBARE_STADT\", \"url\": full_url})\n",
    "\n",
    "    # Process all targets\n",
    "    for item in expanded_targets:\n",
    "        url = item['url']\n",
    "        institute_name = item['name']\n",
    "        base_url = url.split('#')[0]\n",
    "\n",
    "        if base_url in processed_urls: continue\n",
    "\n",
    "        print(f\"{institute_name}: {url}\")\n",
    "        soup = get_soup(url)\n",
    "        if not soup: continue\n",
    "\n",
    "        raw_text = soup.get_text()\n",
    "        final_text = clean_text(raw_text, url)\n",
    "\n",
    "        if len(final_text) < 20:\n",
    "            print(\"Content too short (possibly empty or protected).\")\n",
    "            continue\n",
    "\n",
    "        # Create safe filename\n",
    "        path_slug = urlparse(url).path.strip(\"/\").replace(\"/\", \"_\") or \"main\"\n",
    "        query_slug = urlparse(url).query.replace(\"=\", \"-\").replace(\"&\", \"_\")\n",
    "        safe_slug = f\"{path_slug}_{query_slug}\".strip(\"_\")\n",
    "        if not safe_slug: safe_slug = \"main\"\n",
    "\n",
    "        full_name = f\"{institute_name}_{safe_slug}\"[:60]\n",
    "        filename = os.path.join(PDF_STORAGE_PATH, f\"{full_name}.pdf\")\n",
    "\n",
    "        create_pdf(final_text, url, filename)\n",
    "        processed_urls.add(base_url)\n",
    "        count += 1\n",
    "        time.sleep(0.5) # Polite delay\n",
    "\n",
    "    print(f\"\\nFinished. Created {count} PDFs in {PDF_STORAGE_PATH}\")\n",
    "\n",
    "# Run the pipeline\n",
    "# Note: Since we already have the PDFs in './municipal_pdfs', \n",
    "# you can comment this out if you don't want to re-scrape.\n",
    "# execute_data_ingestion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e6ac1",
   "metadata": {},
   "source": [
    "Web scraping and data cleaning completed, pdfs created and ready in the path: /data/municipal_pdfs\n",
    "\n",
    "\n",
    "After creation of the PDFs, next step is create embeddings for Vector DB and KG and metadata tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eab94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 3: VECTOR DATABASE CREATION & PDF EMBEDDING\n",
    "# ==============================================================================\n",
    "# DESCRIPTION: Reads processed PDFs, assigns Knowledge Graph entity tags (E74),\n",
    "# chunks the text, and creates embeddings in ChromaDB.\n",
    "#\n",
    "# CONFIGURATION NOTE:\n",
    "# This script is configured by default to use L2 (Euclidean) distance.\n",
    "# To use Cosine Similarity, uncomment the specific metadata configuration \n",
    "# in the 'Initialize Collection' section below.\n",
    "\n",
    "import os\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. PATH CONFIGURATION\n",
    "# ------------------------------------------------------------------------------\n",
    "if 'DATA_PATH' not in locals() or 'CHROMA_PATH' not in locals():\n",
    "    ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    DATA_PATH = os.path.join(ROOT_DIR, \"data\", \"municipal_pdfs\")\n",
    "    CHROMA_PATH = os.path.join(ROOT_DIR, \"chroma_db\")\n",
    "\n",
    "print(f\"[CONFIG] PDF Source: {DATA_PATH}\")\n",
    "print(f\"[CONFIG] Vector DB Target: {CHROMA_PATH}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. ENTITY MAPPING (PDF FILENAME -> KNOWLEDGE GRAPH GROUP)\n",
    "# ------------------------------------------------------------------------------\n",
    "PDF_TO_E74 = {\n",
    "    \"DIVERSITY_MEDIA\": \"Diversity_Media\",\n",
    "    \"ESSBARE_STADT\": \"essbare_Stadt_Nürnberg_e.V.\",\n",
    "    \"FABLAB_FAU\": \"FAU_FabLab\",\n",
    "    \"FABLAB_NBG\": \"FabLab_Nürnberg\",\n",
    "    \"FABLAB_NUELAND\": \"FabLab_Nüland\",\n",
    "    \"HEIZHAUS\": \"Heizhaus_Nürnberg\",\n",
    "    \"HOLZWERKSTATT\": \"Holzwerkstatt_Gostenhof_e.V.\",\n",
    "    \"KLARA\": \"KLARA\",\n",
    "    \"KOLEO\": \"KOLEO\",\n",
    "    \"KUNSTKULTUR\": \"KunstKultur_Quartier_Werkstatten\",\n",
    "    \"LEIHLA\": \"Leihla_Nürnberg\",\n",
    "    \"LEONARDO\": \"Leonardo_Zentrum\",\n",
    "    \"ODL_TOLLWERK\": \"tollwerk_GmbH\",\n",
    "    \"OHM_LAB\": \"TH_Nürnberg\",\n",
    "}\n",
    "\n",
    "def infer_e74_group(filename: str) -> str:\n",
    "    for prefix, e74 in PDF_TO_E74.items():\n",
    "        if filename.startswith(prefix + \"_\"):\n",
    "            return e74\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. INITIALIZE CHROMA CLIENT & COLLECTION\n",
    "# ------------------------------------------------------------------------------\n",
    "client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "# --- CONFIGURATION SWITCH ---\n",
    "# Option A: Standard L2 Distance (Default)\n",
    "COLLECTION_NAME = \"municipal_pdfs_rag\"\n",
    "\n",
    "# Option B: Cosine Similarity (Uncomment to use)\n",
    "# COLLECTION_NAME = \"municipal_pdfs_cosine\"\n",
    "\n",
    "try:\n",
    "    client.delete_collection(name=COLLECTION_NAME)\n",
    "    print(f\"[INFO] Deleted existing collection: {COLLECTION_NAME}\")\n",
    "except:\n",
    "    print(f\"[INFO] Collection {COLLECTION_NAME} did not exist. Creating new.\")\n",
    "\n",
    "# Embedding Model\n",
    "ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Create Collection\n",
    "# ---------------------------------------------------------\n",
    "# OPTION A: Standard Creation (L2 Distance) - ACTIVE\n",
    "collection = client.create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_function=ef\n",
    ")\n",
    "\n",
    "# OPTION B: Cosine Similarity Configuration - INACTIVE\n",
    "# Uncomment the block below to enable Cosine Similarity\n",
    "# collection = client.create_collection(\n",
    "#     name=COLLECTION_NAME,\n",
    "#     embedding_function=ef,\n",
    "#     metadata={\"hnsw:space\": \"cosine\"} # Critical for Cosine Similarity\n",
    "# )\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. CHUNKING CONFIGURATION\n",
    "# ------------------------------------------------------------------------------\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. EXECUTION: PROCESS & EMBED\n",
    "# ------------------------------------------------------------------------------\n",
    "def build_vector_database():\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        print(f\"[ERROR] PDF directory not found: {DATA_PATH}\")\n",
    "        return\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(DATA_PATH) if f.endswith(\".pdf\")]\n",
    "    print(f\"[INFO] Found {len(pdf_files)} PDFs. Processing...\")\n",
    "\n",
    "    all_chunks, all_metas, all_ids = [], [], []\n",
    "\n",
    "    for filename in sorted(pdf_files):\n",
    "        file_path = os.path.join(DATA_PATH, filename)\n",
    "        try:\n",
    "            reader = PdfReader(file_path)\n",
    "            full_text = \"\\n\".join([p.extract_text() or \"\" for p in reader.pages])\n",
    "            \n",
    "            if not full_text.strip():\n",
    "                print(f\"[WARNING] Skipping empty file: {filename}\")\n",
    "                continue\n",
    "\n",
    "            chunks = text_splitter.split_text(full_text)\n",
    "            e74_group = infer_e74_group(filename)\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                all_chunks.append(chunk)\n",
    "                all_metas.append({\n",
    "                    \"source_pdf\": filename,\n",
    "                    \"e74_group\": e74_group,\n",
    "                    \"chunk_index\": i\n",
    "                })\n",
    "                all_ids.append(f\"{filename}_chunk_{i}\")\n",
    "            \n",
    "            print(f\"[INFO] Processed: {filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process {filename}: {e}\")\n",
    "\n",
    "    # Upsert to DB\n",
    "    if all_chunks:\n",
    "        print(f\"[INFO] Upserting {len(all_chunks)} chunks into '{COLLECTION_NAME}'...\")\n",
    "        \n",
    "        batch_size = 5000\n",
    "        for i in range(0, len(all_chunks), batch_size):\n",
    "            end = min(i + batch_size, len(all_chunks))\n",
    "            collection.upsert(\n",
    "                documents=all_chunks[i:end],\n",
    "                metadatas=all_metas[i:end],\n",
    "                ids=all_ids[i:end]\n",
    "            )\n",
    "            \n",
    "        print(f\"[SUCCESS] Vector Database populated. Total chunks: {collection.count()}\")\n",
    "    else:\n",
    "        print(\"[WARNING] No chunks found to insert.\")\n",
    "\n",
    "# Uncomment to run the build process\n",
    "# build_vector_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 4: KNOWLEDGE GRAPH EMBEDDING (STRUCTURAL SERIALIZATION)\n",
    "# ==============================================================================\n",
    "# DESCRIPTION: Connects to Neo4j, serializes nodes and relationships into \n",
    "# natural language text (Structural Embedding), and stores them in a \n",
    "# separate ChromaDB collection with CRITICAL METADATA.\n",
    "#\n",
    "# CONFIGURATION NOTE:\n",
    "# This script is configured by default to use L2 (Euclidean) distance.\n",
    "# To use Cosine Similarity, uncomment the specific metadata configuration \n",
    "# in the 'Initialize Collection' section below.\n",
    "\n",
    "import os\n",
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. CONFIGURATION & CREDENTIALS\n",
    "# ------------------------------------------------------------------------------\n",
    "if 'CHROMA_PATH' not in locals():\n",
    "    ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    CHROMA_PATH = os.path.join(ROOT_DIR, \"chroma_db\")\n",
    "\n",
    "# Neo4j Credentials (Securely loaded from .env)\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USERNAME\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "if not NEO4J_URI or not NEO4J_PASSWORD:\n",
    "    raise ValueError(\"[ERROR] Neo4j credentials missing in .env file.\")\n",
    "\n",
    "# --- CONFIGURATION SWITCH ---\n",
    "# Option A: Standard L2 Distance (Default)\n",
    "COLLECTION_NAME = \"kg_structural_rag\"\n",
    "\n",
    "# Option B: Cosine Similarity (Uncomment to use)\n",
    "# COLLECTION_NAME = \"kg_structural_cosine\"\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(f\"[CONFIG] KG Vector DB Target: {CHROMA_PATH}\")\n",
    "print(f\"[CONFIG] Embedding Model: {MODEL_NAME}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. HELPER CLASSES AND FUNCTIONS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class StructuralEmbeddingFunction(EmbeddingFunction):\n",
    "    \"\"\"\n",
    "    Custom embedding function wrapper for ChromaDB using SentenceTransformers.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name):\n",
    "        print(f\"[INIT] Loading embedding model: {model_name}...\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        return self.model.encode(input, convert_to_numpy=True).tolist()\n",
    "\n",
    "def safe_str(value):\n",
    "    \"\"\"Converts complex types to string for safe metadata storage.\"\"\"\n",
    "    if isinstance(value, list):\n",
    "        return \", \".join(str(v) for v in value)\n",
    "    return str(value)\n",
    "\n",
    "def get_node_name(node):\n",
    "    \"\"\"Resolves the display name of a node.\"\"\"\n",
    "    props = dict(node)\n",
    "    return props.get('name', props.get('title', node.element_id))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. MAIN PIPELINE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def generate_kg_embeddings():\n",
    "    # 1. Initialize Connections\n",
    "    print(f\"[CONN] Connecting to Neo4j at {NEO4J_URI}...\")\n",
    "    try:\n",
    "        driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "        driver.verify_connectivity()\n",
    "        print(\"[SUCCESS] Connected to Neo4j.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Connection Failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize ChromaDB\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "    embedding_func = StructuralEmbeddingFunction(MODEL_NAME)\n",
    "\n",
    "    # 2. Reset Collection\n",
    "    try:\n",
    "        chroma_client.delete_collection(name=COLLECTION_NAME)\n",
    "        print(f\"[INFO] Deleted existing collection: {COLLECTION_NAME}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Create Collection\n",
    "    # ---------------------------------------------------------\n",
    "    # OPTION A: Standard Creation (L2 Distance) - ACTIVE\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        embedding_function=embedding_func\n",
    "    )\n",
    "\n",
    "    # OPTION B: Cosine Similarity Configuration - INACTIVE\n",
    "    # Uncomment the block below to enable Cosine Similarity\n",
    "    # collection = chroma_client.create_collection(\n",
    "    #     name=COLLECTION_NAME,\n",
    "    #     embedding_function=embedding_func,\n",
    "    #     metadata={\"hnsw:space\": \"cosine\"} # Critical for Cosine Similarity\n",
    "    # )\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "\n",
    "    print(\"[PROCESS] Fetching data from Neo4j and serializing...\")\n",
    "\n",
    "    with driver.session() as session:\n",
    "        # ---------------------------------------------------------\n",
    "        # PHASE A: PROCESS NODES (Entity Serialization)\n",
    "        # ---------------------------------------------------------\n",
    "        result_nodes = session.run(\"MATCH (n) RETURN n\")\n",
    "\n",
    "        for record in result_nodes:\n",
    "            node = record[\"n\"]\n",
    "            props = dict(node)\n",
    "            labels = list(node.labels)\n",
    "            label_str = labels[0] if labels else \"Unknown Entity\"\n",
    "            name = get_node_name(node)\n",
    "\n",
    "            # 1. Text Serialization\n",
    "            text_rep = f\"Entity: {name}. Type: {label_str}.\"\n",
    "            for key, value in props.items():\n",
    "                if key not in ['name', 'title', 'uri', 'element_id']:\n",
    "                    if value:\n",
    "                        text_rep += f\" {key.replace('_', ' ')}: {value}.\"\n",
    "\n",
    "            # 2. Metadata (CRITICAL FOR HYBRID LINKING)\n",
    "            meta = {\n",
    "                \"kind\": \"entity\",\n",
    "                \"entity_name\": safe_str(name), \n",
    "                \"labels\": safe_str(label_str),\n",
    "                \"source\": \"neo4j\"\n",
    "            }\n",
    "\n",
    "            documents.append(text_rep)\n",
    "            metadatas.append(meta)\n",
    "            ids.append(f\"node_{node.element_id}\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # PHASE B: PROCESS RELATIONSHIPS (Structural Sentences)\n",
    "        # ---------------------------------------------------------\n",
    "        result_rels = session.run(\"MATCH (n)-[r]->(m) RETURN n, r, m\")\n",
    "\n",
    "        for record in result_rels:\n",
    "            source = record[\"n\"]\n",
    "            rel = record[\"r\"]\n",
    "            target = record[\"m\"]\n",
    "\n",
    "            s_name = get_node_name(source)\n",
    "            t_name = get_node_name(target)\n",
    "\n",
    "            # 1. Text Serialization\n",
    "            text_rep = f\"{s_name} is connected to {t_name} via relation {rel.type}.\"\n",
    "\n",
    "            # 2. Metadata (CRITICAL FOR GRAPH CONTEXT)\n",
    "            meta = {\n",
    "                \"kind\": \"relationship\",\n",
    "                \"source_node\": safe_str(s_name),\n",
    "                \"target_node\": safe_str(t_name),\n",
    "                \"relation_type\": safe_str(rel.type),\n",
    "                \"source\": \"neo4j\"\n",
    "            }\n",
    "\n",
    "            documents.append(text_rep)\n",
    "            metadatas.append(meta)\n",
    "            ids.append(f\"rel_{rel.element_id}\")\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    # 3. Batch Ingestion\n",
    "    total_docs = len(documents)\n",
    "    if total_docs > 0:\n",
    "        print(f\"[INFO] Ingesting {total_docs} vectors into ChromaDB...\")\n",
    "        batch_size = 500\n",
    "\n",
    "        for i in range(0, total_docs, batch_size):\n",
    "            end_idx = min(i + batch_size, total_docs)\n",
    "            collection.add(\n",
    "                documents=documents[i:end_idx],\n",
    "                metadatas=metadatas[i:end_idx],\n",
    "                ids=ids[i:end_idx]\n",
    "            )\n",
    "\n",
    "        print(\"[SUCCESS] Knowledge Graph Embedding Complete.\")\n",
    "        print(f\"[STATUS] Total Records in '{COLLECTION_NAME}': {collection.count()}\")\n",
    "    else:\n",
    "        print(\"[WARNING] No data found to process.\")\n",
    "\n",
    "# Uncomment to run the build process\n",
    "# generate_kg_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 5: DUAL-CHANNEL RETRIEVAL TEST (PDF + KG)\n",
    "# ==============================================================================\n",
    "# DESCRIPTION: Executes a hybrid search query across both Vector Databases\n",
    "# (Unstructured PDF Text + Structured Knowledge Graph) to verify retrieval\n",
    "# quality before passing context to the LLM.\n",
    "\n",
    "import os\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from pprint import pprint\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. CONFIGURATION & PATHS\n",
    "# ------------------------------------------------------------------------------\n",
    "if 'CHROMA_PATH' not in locals():\n",
    "    ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    CHROMA_PATH = os.path.join(ROOT_DIR, \"chroma_db\")\n",
    "\n",
    "# --- COLLECTION CONFIGURATION (L2 vs COSINE SWITCH) ---\n",
    "\n",
    "# 1. PDF Collection Settings\n",
    "PDF_COLLECTION_NAME = \"municipal_pdfs_rag\"       # Default (L2)\n",
    "# PDF_COLLECTION_NAME = \"municipal_pdfs_cosine\"  # Option (Cosine)\n",
    "\n",
    "# 2. KG Collection Settings\n",
    "KG_COLLECTION_NAME = \"kg_structural_rag\"         # Default (L2)\n",
    "# KG_COLLECTION_NAME = \"kg_structural_cosine\"    # Option (Cosine)\n",
    "\n",
    "TOP_K_BOTH = 5\n",
    "\n",
    "print(f\"[CONFIG] Retrieval Path: {CHROMA_PATH}\")\n",
    "print(f\"[CONFIG] PDF Collection: {PDF_COLLECTION_NAME}\")\n",
    "print(f\"[CONFIG] KG Collection:  {KG_COLLECTION_NAME}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. RETRIEVAL FUNCTION\n",
    "# ------------------------------------------------------------------------------\n",
    "def run_dual_channel_query(question: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Executes dual-channel search, retrieves context, and formats documents\n",
    "    with explicit source tags (PDF/KG) and entity names for clear verification.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=======================================================================\")\n",
    "    print(f\"QUERY: {question}\")\n",
    "    print(\"=======================================================================\")\n",
    "\n",
    "    try:\n",
    "        client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "        \n",
    "        # Define Embedding Functions to match what was used during ingestion\n",
    "        # PDF Model: paraphrase-multilingual-mpnet-base-v2\n",
    "        ef_pdf = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "        )\n",
    "        \n",
    "        # KG Model: all-MiniLM-L6-v2\n",
    "        ef_kg = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "        # Connect to collections\n",
    "        pdf_collection = client.get_collection(name=PDF_COLLECTION_NAME, embedding_function=ef_pdf)\n",
    "        kg_collection = client.get_collection(name=KG_COLLECTION_NAME, embedding_function=ef_kg)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Connection to ChromaDB failed: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "    # --- STEP 1: RETRIEVAL FROM PDF CORPUS ---\n",
    "    print(f\"[1/2] Searching PDF Corpus (Unstructured Text, K={TOP_K_BOTH})...\")\n",
    "\n",
    "    pdf_results = pdf_collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=TOP_K_BOTH,\n",
    "        include=['documents', 'metadatas']\n",
    "    )\n",
    "\n",
    "    # 1. Label and Format PDF Documents\n",
    "    pdf_context_list = []\n",
    "    if pdf_results['documents'] and pdf_results['documents'][0]:\n",
    "        for doc, meta in zip(pdf_results['documents'][0], pdf_results['metadatas'][0]):\n",
    "            entity_name = meta.get('e74_group', 'UNKNOWN_ENTITY')\n",
    "            # Prepend the source tag directly to the document text\n",
    "            pdf_context_list.append(f\"[SOURCE: PDF | GROUP: {entity_name}] {doc}\")\n",
    "    else:\n",
    "        print(\"   -> No results found in PDF corpus.\")\n",
    "\n",
    "\n",
    "    # --- STEP 2: RETRIEVAL FROM KG CORPUS ---\n",
    "    print(f\"[2/2] Searching KG Corpus (Structural Facts, K={TOP_K_BOTH})...\")\n",
    "\n",
    "    kg_results = kg_collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=TOP_K_BOTH,\n",
    "        include=['documents', 'metadatas']\n",
    "    )\n",
    "\n",
    "    # 2. Label and Format KG Documents\n",
    "    kg_context_list = []\n",
    "    if kg_results['documents'] and kg_results['documents'][0]:\n",
    "        for doc, meta in zip(kg_results['documents'][0], kg_results['metadatas'][0]):\n",
    "            # Get the entity name, prioritizing entity name for nodes, or source node for rels\n",
    "            entity_name = meta.get('entity_name') or meta.get('source_node', 'N/A')\n",
    "            # Prepend the source tag directly to the document text\n",
    "            kg_context_list.append(f\"[SOURCE: KG | ENTITY: {entity_name}] {doc}\")\n",
    "    else:\n",
    "        print(\"   -> No results found in KG corpus.\")\n",
    "\n",
    "\n",
    "    # --- STEP 3: MERGE CONTEXT ---\n",
    "    final_context_list = pdf_context_list + kg_context_list\n",
    "\n",
    "    compiled_context = {\n",
    "        \"Combined_Context\": final_context_list,\n",
    "        \"Total_Documents\": len(final_context_list)\n",
    "    }\n",
    "\n",
    "    print(\"\\n[3/3] Final Compiled Context Structure (TAGGED Output):\")\n",
    "    # Pretty print just a snippet to avoid flooding the console\n",
    "    if final_context_list:\n",
    "        pprint(final_context_list[:2])\n",
    "        print(f\"... and {len(final_context_list)-2} more items.\")\n",
    "    else:\n",
    "        print(\"No context retrieved.\")\n",
    "        \n",
    "    return compiled_context\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# TEST SUITE\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def test_suite(questions: Dict[str, str]):\n",
    "    for name, q in questions.items():\n",
    "        run_dual_channel_query(q)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    questions = {\n",
    "        \"q1\": \"Which groups provide workshops or tools that support woodworking and fabrication activities? Is there any restriction for using these workshop areas or tools?\",\n",
    "        \"q2\": \"Which publicly accessible spaces are suitable for hosting small civic workshops or dialogue sessions?\",\n",
    "        \"q3\": \"Which places support digitizing personal stories and materials, and producing media (like short films or podcasts)?\",\n",
    "        \"q4\": \"Which municipal facilities are suitable for a 'Repair' or 'Maintenance'?\",\n",
    "        \"q5\": \"Which facilities or groups provide 3D printers or 3D printing workshops? What are their specific types, descriptions, and access restrictions?\"\n",
    "    }\n",
    "    \n",
    "    # Run the test suite\n",
    "    # test_suite(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6d1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 6: LOAD LLM (Meta-Llama-3.1-8B-Instruct)\n",
    "# ==============================================================================\n",
    "# DESCRIPTION: Initializes the Llama-3.1-8B-Instruct model using 4-bit \n",
    "# quantization for memory efficiency. Authenticates using the environment \n",
    "# variable. Configured for GREEDY SEARCH (Deterministic/Temp 0) for RAG.\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. AUTHENTICATION & ENVIRONMENT SETUP\n",
    "# ------------------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"[ERROR] HUGGINGFACEHUB_API_TOKEN not found in environment variables.\")\n",
    "\n",
    "try:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"[INFO] Successfully authenticated with Hugging Face.\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Authentication failed: {e}\")\n",
    "    raise e\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"[ERROR] GPU not detected. CUDA is required.\")\n",
    "\n",
    "print(f\"[INFO] GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. QUANTIZATION CONFIGURATION (4-bit)\n",
    "# ------------------------------------------------------------------------------\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. MODEL & TOKENIZER LOADING\n",
    "# ------------------------------------------------------------------------------\n",
    "print(f\"[INFO] Loading model: {model_id}...\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "    print(\"[SUCCESS] Model loaded successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to load model: {e}\")\n",
    "    raise e\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. PIPELINE INITIALIZATION (Deterministic)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Setting do_sample=False creates a deterministic output (equivalent to Temp 0).\n",
    "# This is ideal for RAG to minimize hallucinations.\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=600,\n",
    "    return_full_text=False,\n",
    "    do_sample=False  # ENABLE GREEDY SEARCH (Temperature = 0)\n",
    ")\n",
    "\n",
    "print(\"[SUCCESS] Text generation pipeline initialized (Greedy Search Mode).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5122384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FINAL STEP: HYBRID RAG PIPELINE (MAIN EXECUTION)\n",
    "# ==============================================================================\n",
    "# DESCRIPTION: The core execution engine of the project. It integrates:\n",
    "# 1. LLM-Based Query Understanding, Intent Analysis & Keyword Extraction\n",
    "# 2. Semantic Vector Retrieval (ChromaDB - PDF & KG)\n",
    "# 3. Structured Graph Retrieval (Neo4j Cypher)\n",
    "# 4. Context-Aware Response Generation (Llama 3.1)\n",
    "#\n",
    "# CONFIGURATION NOTE:\n",
    "# Default settings use L2 Distance. To switch to Cosine Similarity, \n",
    "# uncomment the respective collection names AND the threshold values below.\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. CONFIGURATION & DATABASE INITIALIZATION\n",
    "# ------------------------------------------------------------------------------\n",
    "# Define Paths\n",
    "if 'CHROMA_PATH' not in locals():\n",
    "    ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    CHROMA_PATH = os.path.join(ROOT_DIR, \"chroma_db\")\n",
    "\n",
    "# Define Credentials (from Environment)\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_AUTH = (os.getenv(\"NEO4J_USERNAME\", \"neo4j\"), os.getenv(\"NEO4J_PASSWORD\"))\n",
    "\n",
    "if not NEO4J_URI or not NEO4J_AUTH[1]:\n",
    "    raise ValueError(\"[ERROR] Neo4j credentials missing in environment variables.\")\n",
    "\n",
    "# --- METRIC & COLLECTION CONFIGURATION (L2 vs COSINE SWITCH) ---\n",
    "\n",
    "# OPTION A: L2 Distance (Default - Euclidean)\n",
    "# Lower score is better. Range usually [0, 2] for normalized vectors.\n",
    "PDF_COLLECTION_NAME = \"municipal_pdfs_rag\"\n",
    "KG_COLLECTION_NAME = \"kg_structural_rag\"\n",
    "SIMILARITY_THRESHOLD = 1.6  # High tolerance for L2\n",
    "\n",
    "# OPTION B: Cosine Similarity (Uncomment to use)\n",
    "# Distance = 1 - CosineSimilarity. Lower is better. Range [0, 1].\n",
    "# PDF_COLLECTION_NAME = \"municipal_pdfs_cosine\"\n",
    "# KG_COLLECTION_NAME = \"kg_structural_cosine\"\n",
    "# SIMILARITY_THRESHOLD = 0.4  # Stricter tolerance for Cosine Distance (approx 0.6 similarity)\n",
    "\n",
    "print(f\"[INIT] Connecting to Vector DB at: {CHROMA_PATH}\")\n",
    "print(f\"[INIT] Connecting to Graph DB at: {NEO4J_URI}\")\n",
    "print(f\"[CONFIG] PDF Collection: {PDF_COLLECTION_NAME}\")\n",
    "print(f\"[CONFIG] KG Collection:  {KG_COLLECTION_NAME}\")\n",
    "print(f\"[CONFIG] Distance Threshold: {SIMILARITY_THRESHOLD}\")\n",
    "\n",
    "try:\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "    \n",
    "    # Define Embedding Functions (Must match ingestion models)\n",
    "    ef_pdf = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "    )\n",
    "    ef_kg = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    # Initialize Collections\n",
    "    pdf_collection = chroma_client.get_collection(name=PDF_COLLECTION_NAME, embedding_function=ef_pdf)\n",
    "    kg_vec_collection = chroma_client.get_collection(name=KG_COLLECTION_NAME, embedding_function=ef_kg)\n",
    "    \n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n",
    "    driver.verify_connectivity()\n",
    "    print(\"[SUCCESS] All Databases Connected. Narrative Flow Active.\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Database Connection Failed: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ------------------------------------------------------------------------------\n",
    "def clean_ontology_text(text):\n",
    "    \"\"\"Cleans raw ontology identifiers (e.g., 'E53_Place') into readable text.\"\"\"\n",
    "    if not text: return \"\"\n",
    "    text = re.sub(r'^[A-Z]\\d+_', '', text)\n",
    "    text = re.sub(r'-\\d+$', '', text)\n",
    "    text = text.replace('_', ' ')\n",
    "    text = re.sub(r'(?i)\\s(Facility|Area|Place|Object|Model|type|Group)$', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. DYNAMIC KEYWORD GENERATOR (LLM-BASED INTENT ANALYSIS WITH FEW SHOT LEARNING)\n",
    "# ------------------------------------------------------------------------------\n",
    "def generate_dynamic_search_pattern(question: str):\n",
    "    \"\"\"\n",
    "    Uses the LLM to analyze user intent and extract technical keywords \n",
    "    from the user question for targeted Graph database querying.\n",
    "    \"\"\"\n",
    "    extraction_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a search engine backend. Extract core technical keywords in singular form. \n",
    "    Return ONLY keywords separated by pipes (|). Do not include any other text.\n",
    "    STRICT RULE: Avoid generic words like 'access', 'rules', 'organization', 'policy', 'facility'.\n",
    "    \n",
    "    Q: Where is the nearest room with an MRI scanner?\n",
    "    K: mri_scanner|radiology|room\n",
    "    \n",
    "    Q: I need a large venue for a corporate surgery simulation.\n",
    "    K: surgery|simulation|unit|medical|theatre|hospital\n",
    "\n",
    "    Q: Which airlock is available for EVA suits?\n",
    "    K: airlock|eva_suit|pressure_hatch\n",
    "\n",
    "    Q: I am looking for a specialized hangar for satellite maintenance.\n",
    "    K: satellite_maintenance|hangar|space|engineering|data\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Q: {question}\n",
    "    K:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "    try:\n",
    "        output = text_generation_pipeline(extraction_prompt, max_new_tokens=20, do_sample=False)[0]['generated_text']\n",
    "        raw = output.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].strip().lower()\n",
    "        clean_words = re.sub(r\"[^\\w\\|]\", \" \", raw).split()\n",
    "        forbidden = [\n",
    "            \"facility\", \"facilities\", \"group\", \"groups\", \"workshop\", \"workshops\", \n",
    "            \"based\", \"find\", \"looking\", \"where\", \"which\"\n",
    "        ]\n",
    "        \n",
    "        keywords = [w for w in clean_words if len(w) > 2 and w not in forbidden]\n",
    "        \n",
    "        return \"|\".join(keywords[:5])\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Keyword extraction failed: {e}\")\n",
    "        return \"facility|equipment\"\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. MULTI-SOURCE VECTOR RETRIEVAL\n",
    "# ------------------------------------------------------------------------------\n",
    "def retrieve_all_embeddings(question, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieves semantic context from both PDF (unstructured) and KG (structured) vector stores.\n",
    "    \"\"\"\n",
    "    context_data = {\"pdfs\": [], \"kg_vectors\": []}\n",
    "\n",
    "    try:\n",
    "        # --- PDF COLLECTION QUERY ---\n",
    "        res_pdf = pdf_collection.query(\n",
    "            query_texts=[question],\n",
    "            n_results=top_k,\n",
    "            include=['documents', 'metadatas', 'distances']\n",
    "        )\n",
    "\n",
    "        if res_pdf['documents']:\n",
    "            for doc, meta, dist in zip(res_pdf['documents'][0], res_pdf['metadatas'][0], res_pdf['distances'][0]):\n",
    "                if dist > SIMILARITY_THRESHOLD: continue\n",
    "                group = meta.get('e74_group', 'General Doc')\n",
    "                context_data[\"pdfs\"].append(f\"[{group}]: {doc.strip()}\")\n",
    "\n",
    "        # --- KG COLLECTION QUERY ---\n",
    "        res_kg = kg_vec_collection.query(\n",
    "            query_texts=[question],\n",
    "            n_results=top_k,\n",
    "            include=['documents', 'metadatas', 'distances']\n",
    "        )\n",
    "\n",
    "        if res_kg['documents']:\n",
    "            for doc, dist in zip(res_kg['documents'][0], res_kg['distances'][0]):\n",
    "                if dist > SIMILARITY_THRESHOLD: continue\n",
    "                context_data[\"kg_vectors\"].append(f\"{doc.strip()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Vector Retrieval Error: {e}\")\n",
    "        pass\n",
    "\n",
    "    return context_data\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. SEMANTIC HIERARCHY CYPHER SEARCH\n",
    "# ------------------------------------------------------------------------------\n",
    "def fetch_kg_facts(regex_pattern: str):\n",
    "    \"\"\"\n",
    "    Executes a complex Cypher query to retrieve structured facts based on\n",
    "    semantic hierarchy and keyword matching.\n",
    "    \"\"\"\n",
    "    raw_terms = [t.strip().lower() for t in regex_pattern.split('|') if len(t) > 2]\n",
    "\n",
    "    # Specificity Logic\n",
    "    specific_indicators = [\n",
    "        \"wood\", \"woodworking\", \"carpentry\", \"lathe\", \"saw\", \"drill\", \"workshop\",\n",
    "        \"garden\", \"gardening\", \"agriculture\", \"nature\", \"outdoor\", \"farm\", \n",
    "        \"textile\", \"sew\", \"tailor\",\n",
    "        \"3d_Printer\", \"3d\", \"laser\", \"cnc\", \"metal\", \"milling\", \"fabrication\",\n",
    "        \"audio\", \"video\", \"media\", \"sound\", \"lab\", \"studio\", \"record\", \"acoustic\", \"production\",\n",
    "        \"meeting\", \"conference\", \"seminar\", \"event\", \"hall\", \"social\", \"civic\", \n",
    "        \"community\", \"gathering\", \"room\", \"venue\", \"lecture\", \"exhibition\", \n",
    "        \"gallery\", \"stage\", \"performance\"\n",
    "    ]\n",
    "\n",
    "    generic_terms = [\"tool\", \"equipment\", \"machine\", \"device\", \"facility\", \"workshop\", \"space\"]\n",
    "\n",
    "    is_specific = any(s in term for term in raw_terms for s in specific_indicators)\n",
    "\n",
    "    if is_specific:\n",
    "        search_terms = [t for t in raw_terms if not any(g in t for g in generic_terms)]\n",
    "        if not search_terms: search_terms = raw_terms\n",
    "    else:\n",
    "        search_terms = raw_terms\n",
    "\n",
    "    print(f\"[INFO] Active KG Search Terms: {search_terms}\")\n",
    "\n",
    "    access_logic = \"\"\"\n",
    "    OPTIONAL MATCH (entity)-[:P55_has_current_location]->(direct_loc)\n",
    "    OPTIONAL MATCH (owner)-[:P52i_is_current_owner_of]->(fac:E53_Place)\n",
    "    WITH entity, owner, final_type, coalesce(direct_loc.name, fac.name, \"General Facility\") AS LocName\n",
    "    OPTIONAL MATCH (restriction:E30_Right)-[:P105_right_held_by]->(owner)\n",
    "    WITH entity, owner, final_type, LocName,\n",
    "          coalesce(restriction.name, \"Public Access\") AS AccessStatus\n",
    "    \"\"\"\n",
    "\n",
    "    return_part = \"\"\"\n",
    "    RETURN owner.name AS Org,\n",
    "           LocName AS Loc,\n",
    "           entity.name AS Item,\n",
    "           coalesce(final_type.name, \"Equipment\") AS Cat,\n",
    "           coalesce(entity.P3_has_note, \"\") AS Note,\n",
    "           AccessStatus AS Access\n",
    "    \"\"\"\n",
    "\n",
    "    cypher = f\"\"\"\n",
    "    // BRANCH 1: HIERARCHY CHAIN\n",
    "    MATCH (t:E55_Type)\n",
    "    WHERE any(word IN $terms WHERE toLower(t.name) CONTAINS word)\n",
    "    MATCH (child_type:E55_Type)-[:P127_has_broader_term*0..2]->(t)\n",
    "    MATCH (entity)-[:P2_has_type]->(final_type)\n",
    "    WHERE final_type = child_type\n",
    "    AND (entity:`E22_Human-Made_Object` OR entity:`E24_Physical_Human-Made_Thing` OR entity:`E25_Human-Made_Feature`)\n",
    "    MATCH (owner:E74_Group)-[:P52i_is_current_owner_of]->(entity)\n",
    "    {access_logic}\n",
    "    {return_part}\n",
    "\n",
    "    UNION\n",
    "\n",
    "    // BRANCH 2: MODEL CHAIN\n",
    "    MATCH (t:E55_Type)\n",
    "    WHERE any(word IN $terms WHERE toLower(t.name) CONTAINS word)\n",
    "    MATCH (child_type:E55_Type)-[:P127_has_broader_term*0..2]->(t)\n",
    "    MATCH (entity)-[:P2_has_type]->(:E99_Product_Type)-[:P2_has_type]->(final_type)\n",
    "    WHERE final_type = child_type\n",
    "    MATCH (owner:E74_Group)-[:P52i_is_current_owner_of]->(entity)\n",
    "    {access_logic}\n",
    "    {return_part}\n",
    "\n",
    "    UNION\n",
    "\n",
    "    // BRANCH 3: TEXT CHAIN\n",
    "    MATCH (entity)\n",
    "    WHERE (any(word IN $terms WHERE toLower(entity.name) CONTAINS word)\n",
    "        OR any(word IN $terms WHERE toLower(entity.P3_has_note) CONTAINS word))\n",
    "    AND (entity:`E22_Human-Made_Object` OR entity:`E24_Physical_Human-Made_Thing` OR entity:`E25_Human-Made_Feature`)\n",
    "    OPTIONAL MATCH (entity)-[:P2_has_type*1..2]->(final_type:E55_Type)\n",
    "    MATCH (owner:E74_Group)-[:P52i_is_current_owner_of]->(entity)\n",
    "    {access_logic}\n",
    "    {return_part}\n",
    "    \"\"\"\n",
    "\n",
    "    full_query = f\"\"\"\n",
    "    CALL () {{\n",
    "        {cypher}\n",
    "    }}\n",
    "    WITH Org, Loc, Access, Item, Cat, Note\n",
    "    WITH Org, Loc, replace(Access, '_', ' ') AS CleanAccess, Item, replace(Cat, '_', ' ') AS CleanCat, Note\n",
    "\n",
    "    ORDER BY Item\n",
    "    RETURN Org, Loc, CleanAccess, collect(distinct {{Item: Item, Category: CleanCat, Note: Note}}) as Inventory\n",
    "    ORDER BY size(Inventory) DESC\n",
    "    LIMIT 20\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            r = session.run(full_query, terms=search_terms)\n",
    "            for row in r:\n",
    "                org = clean_ontology_text(row[\"Org\"])\n",
    "                loc = clean_ontology_text(row[\"Loc\"])\n",
    "                access = row[\"CleanAccess\"]\n",
    "\n",
    "                items_with_type = []\n",
    "                for item in row[\"Inventory\"]:\n",
    "                    i_name = clean_ontology_text(item['Item'])\n",
    "                    i_cat = clean_ontology_text(item['Category'])\n",
    "                    \n",
    "                    if i_cat.lower() in i_name.lower():\n",
    "                        items_with_type.append(i_name)\n",
    "                    else:\n",
    "                        items_with_type.append(f\"{i_name} (type: {i_cat})\")\n",
    "\n",
    "                resources_str = \", \".join(items_with_type)\n",
    "                \n",
    "                clean_org = org.lower().replace(\" \", \"\")\n",
    "                clean_loc = loc.lower().replace(\" \", \"\")\n",
    "                \n",
    "                if clean_loc in clean_org or clean_org in clean_loc:\n",
    "                    segment = f\"The group '{org}' provides the following: {resources_str}. Access rule: {access}.\"\n",
    "                else:\n",
    "                    segment = f\"The group '{org}' (located at {loc}) provides the following: {resources_str}. Access rule: {access}.\"\n",
    "                results.append(segment)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Neo4j Query Failed: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. RESPONSE GENERATION (CONTEXT-AWARE NARRATIVE)\n",
    "# ------------------------------------------------------------------------------\n",
    "def generate_full_response(question, pdf_context, kg_vec_context, kg_facts):\n",
    "    \"\"\"\n",
    "    Synthesizes the final answer using the LLM with strict formatting rules.\n",
    "    \"\"\"\n",
    "\n",
    "    if not pdf_context and not kg_vec_context and not kg_facts:\n",
    "        return \"Based on the available municipal data, no relevant facilities or workshops were found matching your specific request.\"\n",
    "\n",
    "    all_narrative = \"\\n\".join(pdf_context) + \"\\n\" + \"\\n\".join(kg_vec_context)\n",
    "    kg_fact_text = \"\\n\".join(kg_facts) if kg_facts else \"NO DIRECT INVENTORY MATCH FOUND IN KG.\"\n",
    "\n",
    "    system_instruction = \"\"\"You are the Nuremberg Municipal Expert. Answer using the provided Context Sources.\n",
    "\n",
    "YOUR TASK: Analyze the user's INTENT and map it to the Inventory using the following GENERAL LOGIC RULES.\n",
    "\n",
    "    --- PART 1: LOGICAL REASONING (THE BRAIN) ---\n",
    "    \n",
    "    1. EVIDENCE VERIFICATION (The \"Specificity\" Rule):\n",
    "       - IF the user asks for a SPECIFIC TOOL or OBJECT: You must find an EXPLICIT mention of that exact object in the source text.\n",
    "       - A general facility category (e.g., \"Workshop\", \"Makerspace\") is NOT sufficient proof that they possess a specific device.\n",
    "       - CONSEQUENCE: If the text does not explicitly list the requested item, DISCARD THE FACILITY immediately. Do not mention it.\n",
    "       \n",
    "    2. FUNCTIONAL COMPATIBILITY (The \"Context\" Rule):\n",
    "       - Analyze the nature of the requested activity (e.g., Social Gathering vs. Industrial Production).\n",
    "       - Ensure the recommended facility's primary environment matches this activity.\n",
    "       - EXCLUSION: Do not recommend noise-heavy or industrial environments for social/quiet activities unless they explicitly list a dedicated event space.\n",
    "\n",
    "    3. RELEVANCE FILTERING (The \"Focus\" Rule):\n",
    "       - IF the user asks for a specific category (e.g., \"Avionics or Flight Instruments\"), ONLY mention items within that technical domain.\n",
    "       - DO NOT mention unrelated subsystems such as landing gear, engine components, or cabin interior, even if they are located in the same hangar or facility.\n",
    "       - SILENTLY OMIT all unrelated inventory parts to maintain strict focus on the user's intent.\n",
    "       - If a facility offers a diverse inventory, ONLY extract and mention the items relevant to the user's current specific question.\n",
    "       - SILENTLY OMIT unrelated departments or tools to keep the answer focused.\n",
    "       - NEVER mention about additional information about unrelated fields. If user asks something about flight, you should only strict to the question. \n",
    "\n",
    "    --- PART 2: FORMATTING & STYLE (THE MOUTH) ---\n",
    "\n",
    "    4. NO LISTS: \n",
    "       - Do NOT use bullet points. Write in continuous, flowing, natural paragraphs.\n",
    "\n",
    "    5. NO REDUNDANCY: \n",
    "       - Avoid repetitive phrasing regarding locations. \n",
    "       - If the Organization Name is identical or highly similar to the Location Name, do not state the location separately.\n",
    "\n",
    "    6. ENTITY CONSOLIDATION:\n",
    "       - If the source data contains multiple segments referring to the same organization, SYNTHESIZE them into a single, coherent description. \n",
    "       - Do not write separate sentences or paragraphs for the same entity.\n",
    "\n",
    "    7. ACCESS TRANSLATION:\n",
    "       - Convert raw access tags into natural language statements (e.g., \"It is open to the public\" instead of \"Public Access\").\n",
    "\n",
    "    8. NO RAW METADATA:\n",
    "       - Identify and remove internal tags (e.g., [Type: ...], [Model: ...]). \n",
    "       - Incorporate the information naturally into the sentence structure without using brackets.\n",
    "       \n",
    "    9. MANDATORY OWNERSHIP STRUCTURE:\n",
    "       - You must ALWAYS present the data in a Parent-Child hierarchy.\n",
    "       - Every specific location, facility, or room must be explicitly linked to its owning Organization or Group.\n",
    "       - NEVER mention a sub-facility in isolation.\n",
    "       - Preferred phrasing: \"[Organization Name]'s [Facility Name]\" or \"The [Facility Name] provided by [Organization Name]\".\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    === SOURCE 1: VERIFIED KG INVENTORY ===\n",
    "    {kg_fact_text}\n",
    "\n",
    "    === SOURCE 2: CONTEXT (PDFs & Vectors) ===\n",
    "    {all_narrative}\n",
    "\n",
    "    === USER QUESTION ===\n",
    "    {question}\n",
    "\n",
    "    Answer in flowing paragraphs based on the RULES. DO NOT use single or double quotation marks around organization names, facilities, or tools. Treat them as proper nouns within the flow. NO BULLET POINTS. NO SUMMARIES.\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system_instruction}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    output = text_generation_pipeline(prompt, max_new_tokens=1000)[0]['generated_text']\n",
    "    \n",
    "    return output.split(\"<|end_header_id|>\")[-1].replace(\"<|eot_id|>\", \"\").strip()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. MAIN EXECUTION LOOP\n",
    "# ------------------------------------------------------------------------------\n",
    "questions = {\n",
    "    \"Q1\": \"Which groups provide workshops or tools that support woodworking activities?\",\n",
    "    \"Q2\": \"Which publicly accessible spaces are suitable for hosting small public meetings?\",\n",
    "    \"Q3\": \"Which places support digitizing personal stories and materials, and producing media (like short films or podcasts)?\",\n",
    "    \"Q4\": \"Which municipal facilities are suitable for a 'Repair' or 'Maintenance' project ?\",\n",
    "    \"Q5\": \"Which facilities or groups provide 3D printers or 3D printing workshops?\"\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING HYBRID RAG EXECUTION CYCLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for q_id, q_text in questions.items():\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"[QUERY] {q_id}: {q_text}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # 1. Intent Analysis & Keyword Extraction\n",
    "    regex = generate_dynamic_search_pattern(q_text)\n",
    "    print(f\"[INFO] Search Pattern: [{regex}]\")\n",
    "\n",
    "    # 2. Vector Retrieval\n",
    "    embeddings_data = retrieve_all_embeddings(q_text, top_k=5)\n",
    "\n",
    "    # 3. Graph Retrieval\n",
    "    structured_facts = fetch_kg_facts(regex)\n",
    "    print(f\"[INFO] Graph Found: {len(structured_facts)} organizations with relevant inventory.\")\n",
    "\n",
    "    # 4. Generation\n",
    "    answer = generate_full_response(\n",
    "        q_text,\n",
    "        embeddings_data['pdfs'],\n",
    "        embeddings_data['kg_vectors'],\n",
    "        structured_facts\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[RESULT] Final Answer:\\n{answer}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXECUTION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
